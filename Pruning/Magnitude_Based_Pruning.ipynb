{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ee3e5a-c1c0-4ed0-9b5c-25ac8a72312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7580dd-f903-4c43-b7e9-72fe60459a75",
   "metadata": {},
   "source": [
    "#### Basic implementation of Magnitude based pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "122b6a5f-dcab-46ca-a02a-ca6a8498a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_based_pruning(model, pruning_percentage):\n",
    "    \"\"\"\n",
    "    Prunes a model by setting the smallest weights to zero based on magnitude.\n",
    "    \n",
    "    Args:\n",
    "        model: tf.keras model to prune\n",
    "        pruning_percentage: percentage of weights to prune (0-100)\n",
    "    Returns:\n",
    "        pruned_model\n",
    "    \"\"\"\n",
    "    # Get all weight tensors (kernels only)\n",
    "    weights = [w for w in model.trainable_weights if 'kernel' in w.name]\n",
    "\n",
    "    if not weights:\n",
    "        print(\"No weights found to prune\")\n",
    "        return model\n",
    "\n",
    "    # Flatten and concatenate all weights\n",
    "    all_weights = tf.concat([tf.reshape(w, [-1]) for w in weights], axis=0)\n",
    "    all_weights_abs = tf.abs(all_weights)\n",
    "\n",
    "    # Compute threshold\n",
    "    threshold = np.percentile(all_weights_abs.numpy(), pruning_percentage)\n",
    "\n",
    "    # Prune weights\n",
    "    total_params = 0\n",
    "    pruned_params = 0\n",
    "\n",
    "    for weight in weights:\n",
    "        weight_shape = weight.shape\n",
    "        weight_flat = tf.reshape(weight, [-1])\n",
    "        mask = tf.cast(tf.abs(weight_flat) > threshold, weight.dtype)\n",
    "        pruned_weights = weight_flat * mask\n",
    "\n",
    "        # Update weights in model\n",
    "        weight.assign(tf.reshape(pruned_weights, weight_shape))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06a63092-9131-4499-a46c-a90e32dfd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_with_fine_tuning(model, x_train, y_train, x_val, y_val,\n",
    "                         final_sparsity=0.8, n_iterations=5, epochs_per_iter=2,\n",
    "                         initial_lr=0.001):\n",
    "    \"\"\"\n",
    "    Complete pruning pipeline with fine-tuning and learning rate reduction\n",
    "    \n",
    "    Args:\n",
    "        model: Compiled Keras model\n",
    "        x_train, y_train: Training data\n",
    "        x_val, y_val: Validation data\n",
    "        final_sparsity: Target sparsity (0-1)\n",
    "        n_iterations: Number of pruning iterations\n",
    "        epochs_per_iter: Fine-tuning epochs per iteration\n",
    "        initial_lr: Initial learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Pruned and fine-tuned model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Initial evaluation:\")\n",
    "    model.evaluate(x_val, y_val, verbose=2)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        current_target = (i + 1) * (final_sparsity / n_iterations)\n",
    "        current_lr = initial_lr * (0.1 ** i)  # Reduce LR by 1/10 each iteration\n",
    "        print(f\"\\nPruning iteration {i+1}/{n_iterations}\")\n",
    "        print(f\"Target sparsity: {current_target:.1%}\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        optimizer.learning_rate = current_lr\n",
    "        \n",
    "        # Prune model\n",
    "        model= magnitude_based_pruning(model, current_target * 100)\n",
    "        \n",
    "        # Fine-tune with reduced learning rate\n",
    "        print(\"Fine-tuning...\")\n",
    "        model.fit(x_train, y_train,\n",
    "                epochs=epochs_per_iter,\n",
    "                validation_data=(x_val, y_val),\n",
    "                verbose=1)\n",
    "    \n",
    "    print(\"\\nFinal evaluation:\")\n",
    "    model.evaluate(x_val, y_val, verbose=2)\n",
    "    print(f\"Final sparsity: {final_sparsity} %\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce02d524-758b-40dd-aeb5-9c529086d3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initial model...\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.8879 - loss: 0.3598 - val_accuracy: 0.9640 - val_loss: 0.1210\n",
      "Initial evaluation:\n",
      "313/313 - 2s - 6ms/step - accuracy: 0.9643 - loss: 0.1117\n",
      "\n",
      "Pruning iteration 1/5\n",
      "Target sparsity: 16.0%\n",
      "Fine-tuning...\n",
      "Epoch 1/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9708 - loss: 0.0919 - val_accuracy: 0.9750 - val_loss: 0.0816\n",
      "Epoch 2/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - accuracy: 0.9811 - loss: 0.0569 - val_accuracy: 0.9742 - val_loss: 0.0780\n",
      "\n",
      "Pruning iteration 2/5\n",
      "Target sparsity: 32.0%\n",
      "Fine-tuning...\n",
      "Epoch 1/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9872 - loss: 0.0393 - val_accuracy: 0.9761 - val_loss: 0.0809\n",
      "Epoch 2/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.9896 - loss: 0.0317 - val_accuracy: 0.9784 - val_loss: 0.0858\n",
      "\n",
      "Pruning iteration 3/5\n",
      "Target sparsity: 48.0%\n",
      "Fine-tuning...\n",
      "Epoch 1/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.9912 - loss: 0.0265 - val_accuracy: 0.9787 - val_loss: 0.0765\n",
      "Epoch 2/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 11ms/step - accuracy: 0.9929 - loss: 0.0226 - val_accuracy: 0.9773 - val_loss: 0.0898\n",
      "\n",
      "Pruning iteration 4/5\n",
      "Target sparsity: 64.0%\n",
      "Fine-tuning...\n",
      "Epoch 1/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9947 - loss: 0.0177 - val_accuracy: 0.9831 - val_loss: 0.0739\n",
      "Epoch 2/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - accuracy: 0.9934 - loss: 0.0206 - val_accuracy: 0.9811 - val_loss: 0.0863\n",
      "\n",
      "Pruning iteration 5/5\n",
      "Target sparsity: 80.0%\n",
      "Fine-tuning...\n",
      "Epoch 1/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9929 - loss: 0.0241 - val_accuracy: 0.9807 - val_loss: 0.0753\n",
      "Epoch 2/2\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9942 - loss: 0.0180 - val_accuracy: 0.9816 - val_loss: 0.0730\n",
      "\n",
      "Final evaluation:\n",
      "313/313 - 2s - 5ms/step - accuracy: 0.9816 - loss: 0.0730\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "# 2. Build and compile model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    \n",
    "# 3. Initial training\n",
    "print(\"Training initial model...\")\n",
    "model.fit(x_train, y_train, epochs=2, validation_split=0.2)\n",
    "    \n",
    "# 4. Prune with fine-tuning\n",
    "pruned_model = prune_with_fine_tuning(\n",
    "    model,\n",
    "    x_train, y_train,\n",
    "    x_test, y_test,\n",
    "    final_sparsity=0.8,\n",
    "    n_iterations=5,\n",
    "    epochs_per_iter=2,\n",
    "    initial_lr=0.001\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tinyml_venv)",
   "language": "python",
   "name": "tinyml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
